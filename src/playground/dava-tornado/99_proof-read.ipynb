{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom FFNN implementation\n",
    "import os\n",
    "import sys\n",
    "# Add the parent directory to path to import your modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml, load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Turn down for faster convergence\n",
    "# t0 = time.time()\n",
    "# train_samples = 5000\n",
    "\n",
    "# # Load data from https://www.openml.org/d/554\n",
    "# logging.info(\"Loading data\")\n",
    "# X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# random_state = check_random_state(0)\n",
    "# permutation = random_state.permutation(X.shape[0])\n",
    "# X = X[permutation]\n",
    "# y = y[permutation]\n",
    "# X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, train_size=train_samples, test_size=10000\n",
    "# )\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Turn up tolerance for faster convergence\n",
    "# clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "# clf.fit(X_train, y_train)\n",
    "# sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "# score = clf.score(X_test, y_test)\n",
    "# # print('Best C % .4f' % clf.C_)\n",
    "# print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "# print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "# Create Dummy Data\n",
    "# Set parameters\n",
    "n_samples = 1000  # Number of samples\n",
    "n_features = 4   # Number of features (same as digits dataset)\n",
    "n_classes = 5    # Number of target classes\n",
    "\n",
    "# Generate random feature data\n",
    "X = np.random.randn(n_samples, n_features)  # Random numbers following normal distribution\n",
    "\n",
    "# Generate random integer labels (0-9)\n",
    "y = np.random.randint(0, n_classes, size=n_samples)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for FFNN\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert labels to one-hot encoding for neural network\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create validation set\n",
    "X_train_ffnn, X_val, Y_train_ffnn, Y_val = train_test_split(\n",
    "    X_train, y_train_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training set: {X_train_ffnn.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Input features: {X_train.shape[1]}\")\n",
    "print(f\"Output classes: {y_train_onehot.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glossary\n",
    "\n",
    "# Base configuration for the FFNN\n",
    "base_config = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,  \n",
    "    'loss_function': CCE(),\n",
    "    'activation': Softmax(),\n",
    "    'initializer': NormalInitializer(seed=42)\n",
    "}\n",
    "\n",
    "\n",
    "# Depth Variation\n",
    "# [Input, Hidden Layers..., Output]\n",
    "# Defines the number of hidden layers by varying the length of the list.\n",
    "# Each hidden layer has the same number of neurons.\n",
    "depth_variations = [\n",
    "    [4, 3, 5],         # Shallow network (1 hidden layer with 64 neurons)\n",
    "    [4, 3, 3, 5],     # Medium-depth network (2 hidden layers, each with 64 neurons)\n",
    "    [4, 3, 3, 3, 5]  # Deep network (3 hidden layers, each with 64 neurons)\n",
    "]\n",
    "\n",
    "# Width Variation\n",
    "# [Input, Hidden, Output]\n",
    "# Defines the number of neurons in each layer while keeping the depth constant (1 hidden layer).\n",
    "width_variations = [\n",
    "    [4, 3, 5],   # Narrow network (fewer neurons in the hidden layer)\n",
    "    [4, 5, 5],  # Medium-width network\n",
    "    [4, 7, 5]   # Wide network (more neurons in the hidden layer)\n",
    "]\n",
    "\n",
    "# Activation Variation\n",
    "# [Input, Hidden, Output]\n",
    "# Defines the activation function for each layer.\n",
    "# The number of activation functions must one less than the number of layers.\n",
    "activation_variations = [\n",
    "    [Linear(), Softmax()],\n",
    "    [Sigmoid(), Sigmoid(), Sigmoid()],\n",
    "    [ReLU(), Sigmoid(), Tanh(), Softmax()]\n",
    "]\n",
    "\n",
    "# Loss Function Variation\n",
    "# Defines the loss function to be used for training.\n",
    "loss_function_variations = [\n",
    "    MSE(),\n",
    "    BCE(),\n",
    "    CCE()\n",
    "]\n",
    "\n",
    "# Weight Initialization Variation\n",
    "# Defines the weight initialization strategy for each layer.\n",
    "# The number of initializers must one less than the number of layers.\n",
    "initializer_variations = [\n",
    "    [ZeroInitializer(), ZeroInitializer()],\n",
    "    [ZeroInitializer(), UniformInitializer(low=-1, high=1, seed=22), ZeroInitializer()],\n",
    "    [ZeroInitializer(), NormalInitializer(mean=0.0, var=0.1, seed=22), ZeroInitializer(), ZeroInitializer()]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN Gettings Started\n",
    "\n",
    "# 1. Create Neural Network\n",
    "network = NeuralNetwork(\n",
    "    node_counts=depth_variations[1],\n",
    "    activations=activation_variations[1],\n",
    "    loss_function=loss_function_variations[1],\n",
    "    initialize_methods=initializer_variations[1]\n",
    ")\n",
    "\n",
    "# 2. Create FFNN model\n",
    "ffnn_model = FFNN(network)\n",
    "\n",
    "# 3. Train the model\n",
    "train_history = ffnn_model.fit(\n",
    "    x_train=X_train_ffnn, \n",
    "    y_train=Y_train_ffnn, \n",
    "    batch_size=base_config['batch_size'], \n",
    "    epochs=base_config['epochs'], \n",
    "    validation_data=(X_val, Y_val),\n",
    "    learning_rate=base_config['learning_rate'], \n",
    "    verbose=1)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "y_pred = ffnn_model.predict(X_test)\n",
    "\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test_onehot, axis=1)\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train history is a dictionary containing the loss and accuracy metrics\n",
    "# train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_model.network.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_model.network.plot_weights([1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the gradients of the weights of the first and last layer\n",
    "ffnn_model.network.plot_gradients([1, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
