{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import necessary dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.repository import get_mnist_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.loss import *\n",
    "from lib.activation import *\n",
    "from lib.neural import *\n",
    "from lib.ffnn import *\n",
    "from lib.weight_initializer import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading MNIST data...\")\n",
    "mnist_data = get_mnist_data()\n",
    "print(mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_data[\"X\"]\n",
    "y = mnist_data[\"y\"]\n",
    "X = X / 255.0  \n",
    "\n",
    "# Convert string labels to integers if needed\n",
    "y_int = np.array(y).astype(int) if isinstance(y[0], str) else np.array(y, dtype=int)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding for custom neural network\n",
    "num_classes = 10  # For MNIST\n",
    "y_train_onehot = np.zeros((len(y_train), num_classes))\n",
    "y_train_onehot[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "y_val_onehot = np.zeros((len(y_val), num_classes))\n",
    "y_val_onehot[np.arange(len(y_val)), y_val] = 1\n",
    "\n",
    "y_test_onehot = np.zeros((len(y_test), num_classes))\n",
    "y_test_onehot[np.arange(len(y_test)), y_test] = 1\n",
    "\n",
    "# Common parameters\n",
    "architecture = [784, 128, 10]\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# # Function to evaluate models\n",
    "# def evaluate_model(model, X, y_onehot):\n",
    "#     predictions = model.predict(X)\n",
    "#     predicted_classes = np.argmax(predictions, axis=1)\n",
    "#     true_classes = np.argmax(y_onehot, axis=1)\n",
    "#     return np.mean(predicted_classes == true_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ffnn = X_train  # Add any specific preprocessing here if needed\n",
    "\n",
    "# Create our custom FFNN\n",
    "activations = [ReLU()] + [Softmax()]\n",
    "network = NeuralNetwork(\n",
    "    node_counts=architecture,\n",
    "    activations=activations,\n",
    "    loss_function=CCE(),\n",
    "    initialize_methods=NormalInitializer(seed=42)\n",
    ")\n",
    "custom_model = FFNN(network)\n",
    "\n",
    "# Train our custom model\n",
    "print(\"Training custom FFNN model...\")\n",
    "custom_history = custom_model.fit(\n",
    "    X_train_ffnn, y_train_onehot,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    learning_rate=learning_rate,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate custom model\n",
    "custom_accuracy = evaluate_model(custom_model, X_test, y_test_onehot)\n",
    "print(f\"Custom FFNN accuracy: {custom_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 128  \n",
    "output_size = 10   \n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "sklearn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(hidden_size,),  \n",
    "    activation='relu',                  \n",
    "    solver='sgd',                       \n",
    "    learning_rate_init=learning_rate,   \n",
    "    alpha=0.0,\n",
    "    batch_size=batch_size,              \n",
    "    max_iter=epochs,                    \n",
    "    random_state=42,                    \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Train sklearn model\n",
    "print(\"Training sklearn MLPClassifier...\")\n",
    "print(f\"Architecture: Input({input_size}) -> Hidden({hidden_size}) -> Output({output_size})\")\n",
    "print(f\"Parameters: learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "# Fit the model\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate sklearn model\n",
    "sklearn_predictions = sklearn_model.predict(X_test)\n",
    "sklearn_accuracy = np.mean(sklearn_predictions == y_test)\n",
    "print(f\"\\nsklearn MLP accuracy: {sklearn_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Model Performance Summary -----\n",
      "Metric               Value     \n",
      "------------------------------\n",
      "Accuracy             0.9117142857\n",
      "Precision (macro)    0.9101609981\n",
      "Recall (macro)       0.9103636133\n",
      "F1 Score (macro)     0.9100799392\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(custom_model, X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison plots for custom FFNN vs sklearn MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 1. Accuracy Comparison Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracies = [custom_accuracy, sklearn_accuracy]\n",
    "plt.bar(['Custom FFNN', 'sklearn MLPClassifier'], accuracies, color=['#3498db', '#e74c3c'])\n",
    "plt.title('Model Accuracy Comparison', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0, 1.0)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Custom FFNN Confusion Matrix\n",
    "custom_predictions = np.argmax(custom_model.predict(X_test), axis=1)\n",
    "custom_cm = confusion_matrix(np.argmax(y_test_onehot, axis=1), custom_predictions)\n",
    "im1 = axes[0].imshow(custom_cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0].set_title('Custom FFNN Confusion Matrix', fontsize=15)\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "axes[0].set_xticks(np.arange(10))\n",
    "axes[0].set_yticks(np.arange(10))\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = axes[0].text(j, i, custom_cm[i, j],\n",
    "                          ha=\"center\", va=\"center\", \n",
    "                          color=\"white\" if custom_cm[i, j] > custom_cm.max()/2 else \"black\")\n",
    "\n",
    "# sklearn MLPClassifier Confusion Matrix\n",
    "sklearn_cm = confusion_matrix(y_test, sklearn_predictions)\n",
    "im2 = axes[1].imshow(sklearn_cm, interpolation='nearest', cmap='Blues')\n",
    "axes[1].set_title('sklearn MLPClassifier Confusion Matrix', fontsize=15)\n",
    "fig.colorbar(im2, ax=axes[1])\n",
    "axes[1].set_xticks(np.arange(10))\n",
    "axes[1].set_yticks(np.arange(10))\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = axes[1].text(j, i, sklearn_cm[i, j],\n",
    "                          ha=\"center\", va=\"center\",\n",
    "                          color=\"white\" if sklearn_cm[i, j] > sklearn_cm.max()/2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Per-class accuracy comparison\n",
    "custom_per_class = np.diag(custom_cm) / np.sum(custom_cm, axis=1)\n",
    "sklearn_per_class = np.diag(sklearn_cm) / np.sum(sklearn_cm, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "width = 0.4\n",
    "x = np.arange(10)\n",
    "plt.bar(x - width/2, custom_per_class, width, label='Custom FFNN', color='#3498db')\n",
    "plt.bar(x + width/2, sklearn_per_class, width, label='sklearn MLPClassifier', color='#e74c3c')\n",
    "plt.xlabel('Digit Class', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Per-Class Accuracy Comparison', fontsize=15)\n",
    "plt.xticks(x, [str(i) for i in range(10)])\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. ROC curves for each class (One-vs-Rest)\n",
    "# Binarize the test labels for ROC curve calculation\n",
    "y_test_bin = label_binarize(np.argmax(y_test_onehot, axis=1), classes=range(10))\n",
    "\n",
    "# Get prediction probabilities\n",
    "custom_probs = custom_model.predict(X_test)\n",
    "sklearn_probs = sklearn_model.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Select classes to display to avoid overcrowding (e.g., 0, 1, 2, 8)\n",
    "classes_to_show = [0, 1, 2, 8]\n",
    "\n",
    "for i in classes_to_show:\n",
    "    # Custom FFNN\n",
    "    fpr_custom, tpr_custom, _ = roc_curve(y_test_bin[:, i], custom_probs[:, i])\n",
    "    roc_auc_custom = auc(fpr_custom, tpr_custom)\n",
    "    plt.plot(fpr_custom, tpr_custom, lw=2, alpha=0.7,\n",
    "             label=f'Custom ROC Class {i} (AUC = {roc_auc_custom:.2f})')\n",
    "    \n",
    "    # sklearn MLP\n",
    "    fpr_sklearn, tpr_sklearn, _ = roc_curve(y_test_bin[:, i], sklearn_probs[:, i])\n",
    "    roc_auc_sklearn = auc(fpr_sklearn, tpr_sklearn)\n",
    "    plt.plot(fpr_sklearn, tpr_sklearn, lw=2, alpha=0.7, linestyle='--',\n",
    "             label=f'sklearn ROC Class {i} (AUC = {roc_auc_sklearn:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves (One-vs-Rest)', fontsize=15)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Learning curves (if custom model history is available)\n",
    "if hasattr(custom_history, 'history') and 'loss' in custom_history.history:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot training & validation loss values for custom model\n",
    "    plt.plot(custom_history.history['loss'], label='Custom Train Loss', linewidth=2)\n",
    "    if 'val_loss' in custom_history.history:\n",
    "        plt.plot(custom_history.history['val_loss'], label='Custom Validation Loss', linewidth=2)\n",
    "        \n",
    "    # Add loss values from sklearn model's loss curve if available\n",
    "    if hasattr(sklearn_model, 'loss_curve_'):\n",
    "        plt.plot(sklearn_model.loss_curve_, label='sklearn Loss', linewidth=2, linestyle='--')\n",
    "    \n",
    "    plt.title('Model Loss Curves', fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Misclassification examples\n",
    "# Find indices of examples where the models disagree\n",
    "disagreement_indices = np.where(custom_predictions != sklearn_predictions)[0]\n",
    "\n",
    "if len(disagreement_indices) > 0:\n",
    "    # Select a few examples to display\n",
    "    num_examples = min(5, len(disagreement_indices))\n",
    "    selected_indices = disagreement_indices[:num_examples]\n",
    "    \n",
    "    plt.figure(figsize=(15, 3*num_examples))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        true_label = np.argmax(y_test_onehot[idx])\n",
    "        custom_pred = custom_predictions[idx]\n",
    "        sklearn_pred = sklearn_predictions[idx]\n",
    "        \n",
    "        # Reshape image for display\n",
    "        img = X_test[idx].reshape(28, 28)\n",
    "        \n",
    "        plt.subplot(num_examples, 1, i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'True: {true_label}, Custom: {custom_pred}, sklearn: {sklearn_pred}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Summary statistics table\n",
    "print(\"\\n----- Model Comparison Summary -----\")\n",
    "print(f\"{'Metric':<25} {'Custom FFNN':<15} {'sklearn MLP':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Accuracy':<25} {custom_accuracy:<15.4f} {sklearn_accuracy:<15.4f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# For custom model\n",
    "custom_precision = precision_score(np.argmax(y_test_onehot, axis=1), custom_predictions, average='macro')\n",
    "custom_recall = recall_score(np.argmax(y_test_onehot, axis=1), custom_predictions, average='macro')\n",
    "custom_f1 = f1_score(np.argmax(y_test_onehot, axis=1), custom_predictions, average='macro')\n",
    "\n",
    "# For sklearn model\n",
    "sklearn_precision = precision_score(y_test, sklearn_predictions, average='macro')\n",
    "sklearn_recall = recall_score(y_test, sklearn_predictions, average='macro')\n",
    "sklearn_f1 = f1_score(y_test, sklearn_predictions, average='macro')\n",
    "\n",
    "print(f\"{'Precision (macro)':<25} {custom_precision:<15.4f} {sklearn_precision:<15.4f}\")\n",
    "print(f\"{'Recall (macro)':<25} {custom_recall:<15.4f} {sklearn_recall:<15.4f}\")\n",
    "print(f\"{'F1 Score (macro)':<25} {custom_f1:<15.4f} {sklearn_f1:<15.4f}\")\n",
    "print(\"-\" * 55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
