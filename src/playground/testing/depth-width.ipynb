{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Add the parent directory to path to import your modules\n",
    "os.chdir(\"../..\")\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from lib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 5000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(\n",
    "        coef[i].reshape(28, 28),\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=plt.cm.RdBu,\n",
    "        vmin=-scale,\n",
    "        vmax=scale,\n",
    "    )\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel(\"Class %i\" % i)\n",
    "plt.suptitle(\"Classification vector for...\")\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print(\"Example run in %.3f s\" % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding for neural network\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Input features: {X_train.shape[1]}\")\n",
    "print(f\"Output classes: {y_train_onehot.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengaruh Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define depth variations with 3, 15, and 45 layers\n",
    "depth_variations = [\n",
    "    [784, 156, 10],\n",
    "    [784, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 10],\n",
    "    [784, 156, 156, 156, 156, 156,  156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156,  156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 10]\n",
    "]\n",
    "\n",
    "\n",
    "# Define width variations with 100, 200, 300 neurons per layer\n",
    "width_variations = [\n",
    "    [784, 100, 10],\n",
    "    [784, 200, 10],\n",
    "    [784, 300, 10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network\n",
    "depth_3_network = NeuralNetwork(\n",
    "    node_counts = depth_variations[0],\n",
    "    activations = [Sigmoid()] * (len(depth_variations[0]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")\n",
    "\n",
    "depth_15_network = NeuralNetwork(\n",
    "    node_counts = depth_variations[1],\n",
    "    activations = [Sigmoid()] * (len(depth_variations[1]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")\n",
    "\n",
    "depth_45_network = NeuralNetwork(\n",
    "    node_counts = depth_variations[2],\n",
    "    activations = [Sigmoid()] * (len(depth_variations[2]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FFNN model\n",
    "ffnn_depth_3 = FFNN(depth_3_network)\n",
    "\n",
    "ffnn_depth_15 = FFNN(depth_15_network)\n",
    "\n",
    "ffnn_depth_45 = FFNN(depth_45_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "depth_3_history = ffnn_depth_3.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "depth_15_history = ffnn_depth_15.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "depth_45_history = ffnn_depth_45.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(ffnn_depth_3, X_test, y_test_onehot)\n",
    "\n",
    "evaluate_model(ffnn_depth_15, X_test, y_test_onehot)\n",
    "\n",
    "evaluate_model(ffnn_depth_45, X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plot_training_loss(depth_3_history, \"Depth 3 Training History\")\n",
    "\n",
    "plot_training_loss(depth_15_history, \"Depth 15 Training History\")\n",
    "\n",
    "plot_training_loss(depth_45_history, \"Depth 45 Training History\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengaruh Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network\n",
    "width_100_network = NeuralNetwork(\n",
    "    node_counts = width_variations[0],\n",
    "    activations = [Sigmoid()] * (len(width_variations[0]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")\n",
    "\n",
    "width_200_network = NeuralNetwork(\n",
    "    node_counts = width_variations[1],\n",
    "    activations = [Sigmoid()] * (len(width_variations[1]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")\n",
    "\n",
    "width_300_network = NeuralNetwork(\n",
    "    node_counts = width_variations[2],\n",
    "    activations = [Sigmoid()] * (len(width_variations[2]) - 2) + [Softmax()],\n",
    "    loss_function = MSE(),\n",
    "    initialize_methods = ZeroInitializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FFNN model\n",
    "ffnn_width_100 = FFNN(width_100_network)\n",
    "\n",
    "ffnn_width_200 = FFNN(width_200_network)\n",
    "\n",
    "ffnn_width_300 = FFNN(width_300_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "width_100_history = ffnn_width_100.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "width_200_history = ffnn_width_200.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "width_300_history = ffnn_width_300.fit(\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    learning_rate=0.01,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(ffnn_width_100, X_test, y_test_onehot)\n",
    "\n",
    "evaluate_model(ffnn_width_200, X_test, y_test_onehot)\n",
    "\n",
    "evaluate_model(ffnn_width_300, X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plot_training_loss(width_100_history, \"Width 100 Training History\")\n",
    "\n",
    "plot_training_loss(width_200_history, \"Width 200 Training History\")\n",
    "\n",
    "plot_training_loss(width_300_history, \"Width 300 Training History\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
