// ----------------------------------------------------------------------------------------------------
// Makefile

run:
	@python3 src/playground/main.py

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// .gitignore

__pycache__
.venv

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// README.md

# Tugas-Besar-I-ML

```bash
python3 -m venv .venv
```

<p>Untuk unix user, do</p>

```bash
source .venv/bin/activate
```

<p>Meanwhile for windows, idk. Pastiin pake venv ya cuk


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// requirements.txt

asttokens==3.0.0
comm==0.2.2
contourpy==1.3.1
cycler==0.12.1
debugpy==1.8.13
decorator==5.2.1
exceptiongroup==1.2.2
executing==2.2.0
fonttools==4.56.0
ipykernel==6.29.5
ipython==8.34.0
jedi==0.19.2
joblib==1.4.2
jupyter_client==8.6.3
jupyter_core==5.7.2
kiwisolver==1.4.8
matplotlib==3.10.1
matplotlib-inline==0.1.7
nest-asyncio==1.6.0
networkx==3.4.2
numpy==2.2.3
packaging==24.2
pandas==2.2.3
parso==0.8.4
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.7
prompt_toolkit==3.0.50
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
Pygments==2.19.1
pyparsing==3.2.1
python-dateutil==2.9.0.post0
pytz==2025.1
pyzmq==26.3.0
scikit-learn==1.6.1
scipy==1.15.2
six==1.17.0
stack-data==0.6.3
threadpoolctl==3.6.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
typing_extensions==4.12.2
tzdata==2025.1
wcwidth==0.2.13


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// here



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// plan

class NeuralNetwork:
	layers: List[NetworkLayer]
	weight_matrices: List[Matrix[Int]] # len(ini) = len(layers) - 1	
# Matrix[Int] -> m*n where m = len(layers[i]) and n = len(layers[i+1])
	gradient_matrices: List[Matrix[Int]]
	
	constructor(n: depth)
	
	setLossFunction(f: loss function = MSE)
	
	initialize_weights()
	
	show()
	
	plot_weights(arr: List[Int])
	
	plot_gradients(arr: List[Int])
	
	save(file: str)
	
	load(file: str)
	
	forward_propagation(arr: List[Input]) -> List[Output]

class FFNN:
	construct
	
	
	
	
	

class NetworkLayer:
	constructor(n: banyaknya node, f: fungsi aktivasi)


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// doc/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/neural.py

import numpy as np
import lib.activation as act
import lib.loss as loss
from lib.weight_initializer import WeightInitializer
import random
from typing import List
import pickle
import matplotlib.pyplot as plt
import networkx as nx

class NetworkLayer:
    """A layer in a neural network"""

    nodes: np.ndarray
    activation: act.Activation
    activated_nodes: np.ndarray

    def __init__(self, node_count: int, activation: act.Activation):
        """
        Initialize a network layer
        
        Args:
            node_count: Number of nodes in this layer
            activation: Activation function for this layer
        """
        self.activation = activation
        self.nodes = np.zeros(node_count)
        self.activated_nodes = np.zeros(node_count)



class NeuralNetwork:

    loss_function: loss.Loss
    layers: List[NetworkLayer]
    weights: List[np.ndarray] # 3D array
    gradients: List[np.ndarray] # 3D array
    bias_weights: List[np.ndarray] # 2D array
    bias_gradients: List[np.ndarray] # 2D array

    def __init__(
        self, 
        node_counts: List[int],
        activations: List[act.Activation],
        loss_function: loss.Loss,
        initialize_method: WeightInitializer # TODO: jadiin list (beda per layer)
    ):
        if len(activations) != len(node_counts) - 1:
            raise Exception("Error: tried to declare NeuralNetwork with wrong amount of activation functions.")
        
        if len(node_counts) < 2:
            raise Exception("Error: tried to declare NeuralNetwork with less than 2 layers.")

        self.loss_function = loss_function

        activations = [None] + activations # input layer has no activation

        self.layers = [
            NetworkLayer(cnt, activation)
            for cnt, activation in zip(node_counts, activations)
        ]

        self.initialize_weights(initialize_method)
    
    def initialize_weights(self, initializer: WeightInitializer):
        self.weights = []        
        self.gradients = []
        self.bias_weights = []  
        self.bias_gradients = []  
        
        for i in range(len(self.layers) - 1):
            next_layer_size = len(self.layers[i + 1].nodes)
            current_layer_size = len(self.layers[i].nodes)
            
            weight_matrix = initializer.initialize((next_layer_size, current_layer_size))
            self.weights.append(weight_matrix)
            
            bias_weights = initializer.initialize((next_layer_size,))
            self.bias_weights.append(bias_weights)
            
            self.gradients.append(np.zeros_like(weight_matrix))
            self.bias_gradients.append(np.zeros_like(bias_weights))  

    def show(self): # TODO: add weights (and gradients) display
        """Display the neural network architecture as a graph.
        
        This method visualizes the network structure, including all layers,
        connections between neurons, weights, and gradients.
        """
        
        # Create a directed graph
        G = nx.DiGraph()
        
        # Define positions for all neurons
        pos = {}
        neuron_labels = {}
        
        # Add nodes for each layer
        for layer_idx, layer in enumerate(self.layers):
            n_neurons = len(layer.nodes)
            for neuron_idx in range(n_neurons):
                node_id = f"L{layer_idx}_{neuron_idx}"
                G.add_node(node_id)
                pos[node_id] = (layer_idx, neuron_idx - n_neurons/2)
                
                if layer_idx == 0:
                    neuron_labels[node_id] = f"Input {neuron_idx}"
                elif layer_idx == len(self.layers) - 1:
                    neuron_labels[node_id] = f"Output {neuron_idx}"
                else:
                    neuron_labels[node_id] = f"H{layer_idx}_{neuron_idx}"
        
        # Add edges between neurons
        edge_labels = {}
        for layer_idx in range(len(self.layers) - 1):
            for prev_idx in range(len(self.layers[layer_idx].nodes)):
                for next_idx in range(len(self.layers[layer_idx + 1].nodes)):
                    prev_node = f"L{layer_idx}_{prev_idx}"
                    next_node = f"L{layer_idx + 1}_{next_idx}"
                    
                    weight = self.weights[layer_idx][next_idx, prev_idx]
                    gradient = self.gradients[layer_idx][next_idx, prev_idx]
                    
                    G.add_edge(prev_node, next_node)
                    edge_labels[(prev_node, next_node)] = f"W: {weight:.2f}\nG: {gradient:.2f}"
        
        # Create the plot
        plt.figure(figsize=(12, 8))
        nx.draw(G, pos, with_labels=False, node_size=700, node_color='skyblue', 
                font_weight='bold', arrowsize=20, edge_color='gray')
        
        nx.draw_networkx_labels(G, pos, labels=neuron_labels, font_size=10)
        
        # Draw edge labels (optional, can be messy for large networks)
        if len(self.layers) < 4 and max([len(l.nodes) for l in self.layers]) < 8:
            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
        
        plt.title('Neural Network Architecture')
        plt.axis('off')
        plt.tight_layout()
        plt.show()

    def plot_weights(self, layer_indices=None):
        """Plot the distribution of weights for specified layers.
        
        Args:
            layer_indices: List of indices indicating which layers to plot.
                        If None, all layers are plotted.
                        
        Example:
            # Plot weights for the first and second layers
            model.plot_weights([0, 1])
            
            # Plot weights for all layers
            model.plot_weights()
        """

        
        if layer_indices is None:
            layer_indices = range(len(self.weights))
        
        n_layers = len(layer_indices)
        fig, axes = plt.subplots(1, n_layers, figsize=(n_layers * 4, 4))
        
        # Handle case with only one layer
        if n_layers == 1:
            axes = [axes]
        
        for i, layer_idx in enumerate(layer_indices):
            if layer_idx >= len(self.weights):
                print(f"Warning: Layer index {layer_idx} out of range")
                continue
                
            weights = self.weights[layer_idx].flatten()
            axes[i].hist(weights, bins=30, alpha=0.7)
            axes[i].set_title(f"Layer {layer_idx+1} Weights")
            axes[i].set_xlabel("Weight Value")
            axes[i].set_ylabel("Frequency")
            axes[i].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.show()

    def plot_gradients(self, layer_indices=None):
        """Plot the distribution of gradients for specified layers.
        
        Args:
            layer_indices: List of indices indicating which layers to plot.
                        If None, all layers are plotted.
                        
        Example:
            # Plot gradients for the first and second layers
            model.plot_gradients([0, 1])
            
            # Plot gradients for all layers
            model.plot_gradients()
        """

        
        if layer_indices is None:
            layer_indices = range(len(self.gradients))
        
        n_layers = len(layer_indices)
        fig, axes = plt.subplots(1, n_layers, figsize=(n_layers * 4, 4))
        
        # Handle case with only one layer
        if n_layers == 1:
            axes = [axes]
        
        for i, layer_idx in enumerate(layer_indices):
            if layer_idx >= len(self.gradients):
                print(f"Warning: Layer index {layer_idx} out of range")
                continue
                
            gradients = self.gradients[layer_idx].flatten()
            axes[i].hist(gradients, bins=30, alpha=0.7)
            axes[i].set_title(f"Layer {layer_idx+1} Gradients")
            axes[i].set_xlabel("Gradient Value")
            axes[i].set_ylabel("Frequency")
            axes[i].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def save(self, filepath: str) -> None:
        """Save the neural network model to a file.
        
        Args:
            filepath: Path where the model will be saved
            
        Example:
            # Save the current model
            model.save("my_neural_network.pkl")
        """
        
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, filepath: str) -> 'NeuralNetwork':
        """Load a neural network model from a file.
        
        Args:
            filepath: Path to the saved model file
            
        Returns:
            Loaded neural network model
            
        Example:
            # Load a saved model
            loaded_model = NeuralNetwork.load("my_neural_network.pkl")
        """
        
        with open(filepath, 'rb') as f:
            return pickle.load(f)


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/weight_initializer.py

import numpy as np
from abc import ABC, abstractmethod


class WeightInitializer(ABC):
    """Abstract base class for neural network weight initialization."""
    
    @abstractmethod
    def initialize(self, shape):
        """
        Initialize weights according to a specific distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
        """
        pass


class ZeroInitializer(WeightInitializer):
    """Initialize weights with zeros."""
    
    def initialize(self, shape):
        """
        Initialize weights with zeros.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        Returns:
            numpy.ndarray: Zero-initialized weights.
        """
        return np.zeros(shape)


class UniformInitializer(WeightInitializer):
    """Initialize weights with random values from a uniform distribution."""
    
    def __init__(self, low=-0.05, high=0.05, seed=None):
        """
        Args:
            low (float): Lower bound of the uniform distribution.
            high (float): Upper bound of the uniform distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.low = low
        self.high = high
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a uniform distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.uniform(low=self.low, high=self.high, size=shape)


class NormalInitializer(WeightInitializer):
    """Initialize weights with random values from a normal distribution."""
    
    def __init__(self, mean=0.0, var=0.1, seed=None):
        """
        Args:
            mean (float): Mean of the normal distribution.
            var (float): Variance of the normal distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.mean = mean
        self.std = np.sqrt(var)
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a normal distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.normal(loc=self.mean, scale=self.std, size=shape)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/ffnn.py

from lib.neural import NeuralNetwork, NetworkLayer
from typing import List, Dict, Optional, Tuple
import numpy as np
import time
from tqdm import tqdm
from matplotlib import pyplot as plt
import pickle
from lib.loss import Loss, MSE, BCE, CCE
from lib.activation import Sigmoid, Softmax

class FFNN:
    
    network: NeuralNetwork
    loss_history: Dict[str, List[float]]

    def __init__(self, network: NeuralNetwork):
        """Initialize a Feed-Forward Neural Network.
        
        Args:
            network: The neural network architecture
            learning_rate: Learning rate for gradient descent
        """
        self.network = network
        self.loss_history = {
            'train_loss': [],
            'val_loss': []
        }

    def forward_prop(self, x_batch: np.ndarray) -> np.ndarray:
        """Perform forward propagation through the network.
        
        Args:
            x_batch: Input data batch of shape (batch_size, input_features)
            
        Returns:
            Output predictions of shape (batch_size, output_features)
            
        Example:
            # For a network with 2 input features and 1 output:
            x_sample = np.array([[0.5, 0.8], [0.1, 0.2]])  # 2 samples, 2 features each
            predictions = model.forward_prop(x_sample)
            # predictions shape: (2, 1) e.g., [[0.75], [0.32]]
        """
        # Ensure x_batch is 2D
        if x_batch.ndim == 1:
            x_batch = x_batch.reshape(1, -1)
        
            
        batch_size = x_batch.shape[0]
        
        # Set input layer values
        self.network.layers[0].nodes = x_batch.T  # Transpose to (features, batch_size)
        self.network.layers[0].activated_nodes = self.network.layers[0].nodes  # No activation for input layer
        
        # Forward pass through each layer   
        for i in range(1, len(self.network.layers)):
            current_layer = self.network.layers[i]
            prev_layer = self.network.layers[i-1]
            
            # Compute the weighted sum: weights * prev_activated_nodes + bias
            # Shape becomes (current_layer_nodes, batch_size)
            weighted_sum = np.dot(self.network.weights[i-1], prev_layer.activated_nodes)
            
            # Add bias (broadcasting across batch)
            bias_expanded = self.network.bias_weights[i-1].reshape(-1, 1)
            weighted_sum = weighted_sum + bias_expanded
            
            # Store pre-activation values
            current_layer.nodes = weighted_sum
            
            # Apply activation function
            if current_layer.activation is not None:
                current_layer.activated_nodes = current_layer.activation.function(weighted_sum)
            else:
                current_layer.activated_nodes = weighted_sum
        
        # Return output layer activations transposed back to (batch_size, output_features)
        return self.network.layers[-1].activated_nodes.T
    
    def back_prop(self, x_batch: np.ndarray, y_batch: np.ndarray) -> float:
        """Perform backward propagation to compute gradients.
        
        Args:
            x_batch: Input data batch of shape (batch_size, input_features)
            y_batch: Target data batch of shape (batch_size, output_features)
            
        Returns:
            The computed loss value
            
        Example:
            # For a batch of 10 samples with 5 input features and 3 output classes
            x_batch = np.random.rand(10, 5)  # 10 samples, 5 features each
            y_batch = np.zeros((10, 3))  # One-hot encoded targets for 3 classes
            # Set the correct class for each sample
            for i in range(10):
                y_batch[i, np.random.randint(0, 3)] = 1
                
            # Perform backward propagation
            loss = model.back_prop(x_batch, y_batch)
            # loss is a float representing the loss for this batch
        """
        # Ensure inputs are properly shaped
        if x_batch.ndim == 1:
            x_batch = x_batch.reshape(1, -1)
        if y_batch.ndim == 1:
            y_batch = y_batch.reshape(1, -1)
            
        batch_size = x_batch.shape[0]
        
        # Forward pass (to compute activations)
        self.forward_prop(x_batch)
        
        # Initialize gradients for this batch
        for i in range(len(self.network.weights)):
            self.network.gradients[i] = np.zeros_like(self.network.weights[i])
            self.network.bias_gradients[i] = np.zeros_like(self.network.bias_weights[i])
        
        # Compute output layer error (delta)
        # Shape: (output_features, batch_size)
        output_layer = self.network.layers[-1]
        output_activations = output_layer.activated_nodes  # Shape: (output_features, batch_size)
        
        # Transpose y_batch to match the shape of output_activations
        y_batch_T = y_batch.T  # Shape: (output_features, batch_size)
        
        # Calculate loss
        loss = self.network.loss_function.function(y_batch_T, output_activations)
        
        d_loss = self.network.loss_function.derivative(y_batch_T, output_activations)
        d_activation = output_layer.activation.derivative(output_layer.nodes)


        if (d_activation.shape[0] == d_activation.shape[1]):
            # d_activation will be (output_sz, output_sz, batch_sz)
            # d_loss shape: (output_sz, batch_sz)
            # Expected output delta: (output_sz, batch_sz)
            
            output_sz, batch_sz = d_loss.shape
            delta = np.zeros((output_sz, batch_sz))
            
            for i in range(batch_sz):
                # For each sample in the batch
                # d_activation[:,:,i] is the Jacobian matrix (output_sz, output_sz)
                # d_loss[:,i] is the loss gradient vector (output_sz,)
                delta[:,i] = np.dot(d_activation[:,:,i], d_loss[:,i])
    
        else: 
            delta = d_loss * d_activation

        
        # Backpropagate the error through the network
        for l in reversed(range(1, len(self.network.layers))):
            layer = self.network.layers[l]
            prev_layer = self.network.layers[l-1]
            
            # Compute weight gradients for this layer
            # delta shape: (current_layer_size, batch_size)
            # prev_activations shape: (prev_layer_size, batch_size)
            # gradient shape: (current_layer_size, prev_layer_size)
            self.network.gradients[l-1] = np.dot(delta, prev_layer.activated_nodes.T) / batch_size

            # Compute bias gradients (average across batch)
            self.network.bias_gradients[l-1] = np.mean(delta, axis=1)
            
            # Backpropagate delta to previous layer (if not input layer)
            if l > 1:
                # Compute delta for previous layer
                # delta shape: (current_layer_size, batch_size)
                # weights shape: (current_layer_size, prev_layer_size)
                # new delta shape: (prev_layer_size, batch_size)
                delta = np.dot(self.network.weights[l-1].T, delta)
                delta *= prev_layer.activation.derivative(prev_layer.nodes)
        
        return loss
    
    def update_weights(self, learning_rate) -> None:
        """Update weights using gradient descent.
        
        Example:
            # After computing gradients with back_prop
            model.update_weights()
            
            # This will update all weights and biases in the network:
            # For each weight matrix and bias vector:
            # w_new = w_old - learning_rate * gradient
        """
        for i in range(len(self.network.weights)):
            self.network.weights[i] -= learning_rate * self.network.gradients[i]
            self.network.bias_weights[i] -= learning_rate * self.network.bias_gradients[i]
    
    def fit(self, 
        x_train: np.ndarray, 
        y_train: np.ndarray,
        batch_size: int = 32,
        epochs: int = 10,
        validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,
        learning_rate: float = 0.01,
        verbose: int = 1
    ) -> Dict[str, List[float]]:
        """Train the neural network using batched gradient descent.
        
        Args:
            x_train: Training data of shape (n_samples, n_features)
            y_train: Training targets of shape (n_samples, n_outputs)
            batch_size: Size of mini-batches
            epochs: Number of training epochs
            validation_data: Optional tuple of (x_val, y_val) for validation
            verbose: 0 for no output, 1 for progress bar and metrics
            
        Returns:
            History dictionary containing training and validation loss
            
        Example:
            # For a dataset with 1000 samples, 20 features, and 3 classes
            x_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each
            y_train = np.zeros((1000, 3))       # One-hot encoded targets for 3 classes
            # Set the correct class for each sample
            for i in range(1000):
                y_train[i, np.random.randint(0, 3)] = 1
                
            # Optional validation data
            x_val = np.random.rand(200, 20)
            y_val = np.zeros((200, 3))
            for i in range(200):
                y_val[i, np.random.randint(0, 3)] = 1
                
            # Train the model
            history = model.fit(
                x_train, 
                y_train,
                batch_size=32,
                epochs=50,
                validation_data=(x_val, y_val),
                verbose=1
            )
            
            # history is a dictionary with keys:
            # - 'train_loss': list of training loss values for each epoch
            # - 'val_loss': list of validation loss values for each epoch
        """
        # Ensure inputs are properly shaped
        if x_train.ndim == 1:
            x_train = x_train.reshape(-1, 1)
        if y_train.ndim == 1:
            y_train = y_train.reshape(-1, 1)
            
        num_samples = x_train.shape[0]
        
        # Reset loss history
        self.loss_history = {
            'train_loss': [],
            'val_loss': []
        }
        
        # Training loop
        for epoch in range(epochs):
            start_time = time.time()
            
            # Shuffle training data
            indices = np.random.permutation(num_samples)
            x_shuffled = x_train[indices]
            y_shuffled = y_train[indices]
            
            # Initialize epoch loss
            epoch_loss = 0.0
            
            # Create mini-batches
            num_batches = int(np.ceil(num_samples / batch_size))
            
            # Progress tracking
            if verbose == 1:
                batch_iterator = tqdm(range(num_batches), desc=f"Epoch {epoch+1}/{epochs}")
            else:
                batch_iterator = range(num_batches)
            
            # Process each batch
            for batch_idx in batch_iterator:
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, num_samples)
                
                batch_x = x_shuffled[start_idx:end_idx]
                batch_y = y_shuffled[start_idx:end_idx]
                
                # Forward and backward passes
                batch_loss = self.back_prop(batch_x, batch_y)
                
                # Update weights
                self.update_weights(learning_rate)
                
                # Update epoch loss (weighted by batch size)
                batch_weight = (end_idx - start_idx) / num_samples
                epoch_loss += batch_loss * batch_weight
                
                # Update progress bar
                if verbose == 1:
                    batch_iterator.set_postfix({"loss": f"{epoch_loss:.4f}"})
            
            # Record training loss
            self.loss_history['train_loss'].append(epoch_loss)
            
            # Validation phase
            val_loss = None
            if validation_data is not None:
                x_val, y_val = validation_data
                
                # Ensure proper shapes
                if x_val.ndim == 1:
                    x_val = x_val.reshape(-1, 1)
                if y_val.ndim == 1:
                    y_val = y_val.reshape(-1, 1)
                
                # Forward pass on validation data
                val_predictions = self.predict(x_val)
                
                # Calculate validation loss
                val_loss = self.network.loss_function.function(y_val.T, self.network.layers[-1].activated_nodes)
                self.loss_history['val_loss'].append(val_loss)
            
            # Print epoch summary
            if verbose == 1:
                epoch_time = time.time() - start_time
                if val_loss is not None:
                    print(f"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}")
                else:
                    print(f"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - loss: {epoch_loss:.4f}")
        
        return self.loss_history

    def predict(self, x: np.ndarray) -> np.ndarray:
        """Generate predictions for input samples.
        
        Args:
            x: Input data of shape (n_samples, n_features)
            
        Returns:
            Predictions of shape (n_samples, n_outputs)
            
        Example:
            # For a dataset with 10 samples, 20 features, and a model with 3 output classes
            x_test = np.random.rand(10, 20)
            
            # Make predictions
            predictions = model.predict(x_test)
            # predictions shape: (10, 3), e.g.:
            # [[0.1, 0.7, 0.2],  # Probabilities for sample 1
            #  [0.8, 0.1, 0.1],  # Probabilities for sample 2
            #  ...]
            
            # For classification, get the class with highest probability
            predicted_classes = np.argmax(predictions, axis=1)
            # predicted_classes: [1, 0, ...]
        """
        return self.forward_prop(x)

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> float:
        """Evaluate the model on test data.
        
        Args:
            x: Test data of shape (n_samples, n_features)
            y: True values of shape (n_samples, n_outputs)
            
        Returns:
            Loss value on test data
            
        Example:
            # For a test dataset with 100 samples, 20 features, and 3 classes
            x_test = np.random.rand(100, 20)
            y_test = np.zeros((100, 3))       # One-hot encoded targets
            for i in range(100):
                y_test[i, np.random.randint(0, 3)] = 1
                
            # Evaluate the model
            test_loss = model.evaluate(x_test, y_test)
            # test_loss is a float, e.g., 0.18
        """
        # Ensure inputs are properly shaped
        if x.ndim == 1:
            x = x.reshape(-1, 1)
        if y.ndim == 1:
            y = y.reshape(-1, 1)
            
        # Forward pass
        predictions = self.predict(x)
        
        # Calculate loss
        return self.network.loss_function.function(y.T, self.network.layers[-1].activated_nodes)
    
    def save(self, filepath: str) -> None:
        """Save the model to a file.
        
        Args:
            filepath: Path to save the model
            
        Example:
            # Save the trained model
            model.save("my_mnist_model.pkl")
        """
        with open(filepath, 'wb') as f:
            pickle.dump({
                'network': self.network,
                'learning_rate': self.learning_rate,
                'loss_history': self.loss_history
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'FFNN':
        """Load a model from a file.
        
        Args:
            filepath: Path to the saved model
            
        Returns:
            Loaded FFNN model
            
        Example:
            # Load a previously saved model
            loaded_model = FFNN.load("my_mnist_model.pkl")
            
            # Use the loaded model for predictions
            test_predictions = loaded_model.predict(x_test)
        """
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        model = cls(data['network'], data['learning_rate'])
        model.loss_history = data['loss_history']
        return model
    
    def plot_loss_history(self) -> None:
        """Plot the training and validation loss history.
        
        Example:
            # After training the model
            model.plot_loss_history()
            
            # This will display a graph showing:
            # - Training loss curve (blue line)
            # - Validation loss curve (if validation data was provided, orange line)
            # The x-axis represents epochs, and the y-axis represents loss values
        """
        if not self.loss_history['train_loss']:
            print("No training history to plot.")
            return
            
        plt.figure(figsize=(10, 6))
        plt.plot(self.loss_history['train_loss'], label='Training Loss', marker='o')
        
        if self.loss_history['val_loss']:
            plt.plot(self.loss_history['val_loss'], label='Validation Loss', marker='x')
        
        plt.title('Loss During Training')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/loss.py

import numpy as np
from abc import ABC, abstractmethod

EPSILON = 1e-15

class Loss(ABC):
    @abstractmethod
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        raise NotImplementedError
    
    @abstractmethod
    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        raise NotImplementedError


class MSE(Loss):
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        return np.mean(np.sum((y_true - y_pred) ** 2, axis=0))
    
    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        batch_size = y_true.shape[1] if y_true.ndim > 1 else 1
        return (-2/batch_size) * (y_true - y_pred)


class BCE(Loss):
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        y_pred = np.clip(y_pred, EPSILON, 1.0 - EPSILON)
        batch_size = y_true.shape[1] if y_true.ndim > 1 else 1
        return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / batch_size
    
    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        y_pred = np.clip(y_pred, EPSILON, 1.0 - EPSILON)
        batch_size = y_true.shape[1] if y_true.ndim > 1 else 1
        return (-1/batch_size) * (y_true / y_pred - (1 - y_true) / (1 - y_pred))


class CCE(Loss):
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        y_pred = np.clip(y_pred, EPSILON, 1.0)
        batch_size = y_true.shape[1] if y_true.ndim > 1 else 1
        return -np.sum(y_true * np.log(y_pred)) / batch_size
    
    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        y_pred = np.clip(y_pred, EPSILON, 1.0)
        batch_size = y_true.shape[1] if y_true.ndim > 1 else 1
        return (-1/batch_size) * (y_true / y_pred)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/activation.py

import numpy as np
from abc import ABC, abstractmethod
from typing import Callable


class Activation(ABC):
    """Base class for activation functions"""
    @abstractmethod
    def function(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    @abstractmethod
    def derivative(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    

class Linear(Activation):
    """Linear activation function"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return x
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.ones_like(x)

class ReLU(Activation):
    """ReLU activation function: f(x) = max(0, x)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.maximum(0, x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.where(x > 0, 1, 0)


class Sigmoid(Activation):
    """Sigmoid activation function: f(x) = 1 / (1 + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        fx = self.function(x)
        return fx * (1 - fx)


class Tanh(Activation):
    """Hyperbolic tangent activation function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.tanh(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return 1 - np.tanh(x) ** 2


class Softmax(Activation):
    """Softmax activation function: f(x_i) = e^x_i / sum(e^x_j)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        softmax_output = self.function(x)  
        
        output_sz, batch_sz = softmax_output.shape
        
        result = np.zeros((output_sz, output_sz, batch_sz))
        
        for i in range(batch_sz):
            s = softmax_output[:, i]
            result[:, :, i] = np.diag(s) - np.outer(s, s)
        
        return result

class CustomActivation(Activation):
    """Custom activation function"""
    fn: Callable

    def __init__(self, fn: Callable) -> None:
        self.fn = fn
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return self.fn(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        # TODO: AldyPy
        pass



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/main.py

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import lib.activation
import lib.ffnn
import lib.neural
import lib.loss
import lib.weight_initializer

sigma = lib.activation.Sigmoid()
nn = lib.neural.NeuralNetwork(
    [5, 3, 2, 1],
    [sigma] * 3,
    lib.loss.MSE(),
    lib.weight_initializer.NormalInitializer(1, 0.1, 13522022)
)
nn.plot_gradients(layer_indices=[0,2])

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/services/.gitkeep



// -----------------------------------------------------------------------------------

