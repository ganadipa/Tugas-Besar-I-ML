// ----------------------------------------------------------------------------------------------------
// Makefile

run:
	@python3 src/playground/main.py

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// .gitignore

__pycache__
.venv

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// README.md

# Tugas-Besar-I-ML

```bash
python3 -m venv .venv
```

<p>Untuk unix user, do</p>

```bash
source .venv/bin/activate
```

<p>Meanwhile for windows, idk. Pastiin pake venv ya cuk


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// requirements.txt

contourpy==1.3.1
cycler==0.12.1
fonttools==4.56.0
kiwisolver==1.4.8
matplotlib==3.10.1
numpy==2.2.3
packaging==24.2
pillow==11.1.0
pyparsing==3.2.1
python-dateutil==2.9.0.post0
six==1.17.0


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// here

// ----------------------------------------------------------------------------------------------------
// Makefile

run:
	@python3 src/playground/main.py

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// .gitignore

__pycache__
.venv

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// README.md

# Tugas-Besar-I-ML

```bash
python3 -m venv .venv
```

<p>Untuk unix user, do</p>

```bash
source .venv/bin/activate
```

<p>Meanwhile for windows, idk. Pastiin pake venv ya cuk


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// requirements.txt

contourpy==1.3.1
cycler==0.12.1
fonttools==4.56.0
kiwisolver==1.4.8
matplotlib==3.10.1
numpy==2.2.3
packaging==24.2
pillow==11.1.0
pyparsing==3.2.1
python-dateutil==2.9.0.post0
six==1.17.0


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// plan

class NeuralNetwork:
	layers: List[NetworkLayer]
	weight_matrices: List[Matrix[Int]] # len(ini) = len(layers) - 1	
# Matrix[Int] -> m*n where m = len(layers[i]) and n = len(layers[i+1])
	gradient_matrices: List[Matrix[Int]]
	
	constructor(n: depth)
	
	setLossFunction(f: loss function = MSE)
	
	initialize_weights()
	
	show()
	
	plot_weights(arr: List[Int])
	
	plot_gradients(arr: List[Int])
	
	save(file: str)
	
	load(file: str)
	
	forward_propagation(arr: List[Input]) -> List[Output]

class FFNN:
	construct
	
	
	
	
	

class NetworkLayer:
	constructor(n: banyaknya node, f: fungsi aktivasi)


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// doc/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/neural.py

import numpy as np
import lib.activation as act
import lib.loss as loss
from lib.weight_initializer import WeightInitializer
import random
from typing import List

class NetworkLayer:

    nodes: np.array
    activation: act.Activation
    activated_nodes: np.array

    def __init__(self, node_count: int, activation: act.Activation):
        self.activation = None if activation is None else activation()
        self.nodes = np.array([0] * node_count)
        self.activated_nodes = np.array([0] * node_count)


class NeuralNetwork:

    loss_function: loss.Loss
    layers: List[NetworkLayer]
    weights: List[np.array] # 3D array
    gradients: List[np.array] # 3D array
    bias_weights: List[np.array] # 2D array

    def __init__(
        self, 
        node_counts: List[int],
        activations: List[act.Activation],
        loss_function: loss.Loss,
        initialize_method: WeightInitializer
    ):
        if len(activations) != len(node_counts) - 1:
            raise Exception("Error: tried to declare NeuralNetwork with wrong amount of activation functions.")
        
        if len(node_counts) < 2:
            raise Exception("Error: tried to declare NeuralNetwork with less than 2 layers.")

        self.loss_function = loss_function

        activations = [None] + activations # input layer has no activation

        self.layers = [
            NetworkLayer(cnt, activation)
            for cnt, activation in zip(node_counts, activations)
        ]

        self.initialize_weights(initialize_method)
    
    def initialize_weights(self, initializer: WeightInitializer):

        self.weights = []        
        self.gradients = []
        self.bias_weights = []  
        
        for i in range(len(self.layers) - 1):
            next_layer_size = len(self.layers[i + 1].nodes)
            current_layer_size = len(self.layers[i].nodes)
            
            weight_matrix = initializer.initialize((next_layer_size, current_layer_size))
            self.weights.append(weight_matrix)
            
            bias_weights = initializer.initialize((next_layer_size,))
            self.bias_weights.append(bias_weights)
            
            self.gradients.append(np.zeros_like(weight_matrix))



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/weight_initializer.py

import numpy as np
from abc import ABC, abstractmethod


class WeightInitializer(ABC):
    """Abstract base class for neural network weight initialization."""
    
    @abstractmethod
    def initialize(self, shape):
        """
        Initialize weights according to a specific distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
        """
        pass


class ZeroInitializer(WeightInitializer):
    """Initialize weights with zeros."""
    
    def initialize(self, shape):
        """
        Initialize weights with zeros.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        Returns:
            numpy.ndarray: Zero-initialized weights.
        """
        return np.zeros(shape)


class UniformInitializer(WeightInitializer):
    """Initialize weights with random values from a uniform distribution."""
    
    def __init__(self, low=-0.05, high=0.05, seed=None):
        """
        Args:
            low (float): Lower bound of the uniform distribution.
            high (float): Upper bound of the uniform distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.low = low
        self.high = high
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a uniform distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.uniform(low=self.low, high=self.high, size=shape)


class NormalInitializer(WeightInitializer):
    """Initialize weights with random values from a normal distribution."""
    
    def __init__(self, mean=0.0, var=0.1, seed=None):
        """
        Args:
            mean (float): Mean of the normal distribution.
            var (float): Variance of the normal distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.mean = mean
        self.std = np.sqrt(var)
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a normal distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.normal(loc=self.mean, scale=self.std, size=shape)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/ffnn.py

from lib.neural import NeuralNetwork, NetworkLayer
from typing import List

class FFNN:
    
    network: NeuralNetwork
    learning_rate: float

    def __init__(self, network: NeuralNetwork, learning_rate: float):
        """TODO: implement CLI to input all Neural Network parameters:
            - weight initialization method
            - node counts
            - activation functions
            - loss function

            Uhm I think this must be done in the client
        """
        self.network = network
        self.learning_rate = learning_rate

    def forward_prop(self, ):
        """TODO: implement forward prop by updating:
            - layers[i].nodes[j]
            - layers[i].activated_nodes[j]
            for 1 < i < layers[i].length and 0 < j < layers[i].length

            i.e. calculate output of all nodes of each layer (except input layer)
        """
    
    def back_prop(self, ):
        """TODO: implement backward prop by updating all:
            - weights[k][i][j]
            - gradients[k][i][j]
        """
    
    def update_weights(self, learning_rate: float):
        """TODO: implement update weights by updating all weights after back prop
        """
    
    def fit(self, 
        x_train: List[List[float]], 
        y_train: List[List[float]],
        batch_size: int,
        learning_rate: float,
        epoch_count: int
    ):
        """TODO: implement batch fit
        """

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/loss.py

import numpy as np
from abc import ABC, abstractmethod


class Loss(ABC):
    """Base class for loss functions"""
    @abstractmethod
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    @abstractmethod
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        raise NotImplementedError


class MSE(Loss):
    """Mean Squared Error loss"""
    def __init__(self) -> None:
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return 0.5 * np.mean((y_true - y_pred) ** 2)
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return self.function(y_true, y_pred)


class BCE(Loss):
    """Binary Cross-Entropy loss"""
    def __init__(self) -> None:
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return self.function(y_true, y_pred)


class CCE:
    """Categorical Cross-Entropy loss"""
    def __init__(self):
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
        return loss
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:        
        return self.function(y_true, y_pred)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/activation.py

import numpy as np
from abc import ABC, abstractmethod
from typing import Callable


class Activation(ABC):
    """Base class for activation functions"""
    @abstractmethod
    def function(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    @abstractmethod
    def derivative(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    

class Linear(Activation):
    """Linear activation function"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return x
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.ones_like(x)

class ReLU(Activation):
    """ReLU activation function: f(x) = max(0, x)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.maximum(0, x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.where(x > 0, 1, 0)


class Sigmoid(Activation):
    """Sigmoid activation function: f(x) = 1 / (1 + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        fx = self.function(x)
        return fx * (1 - fx)


class Tanh(Activation):
    """Hyperbolic tangent activation function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.tanh(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return 1 - np.tanh(x) ** 2


class Softmax(Activation):
    """Softmax activation function: f(x_i) = e^x_i / sum(e^x_j)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        softmax_output = self.function(x)
        s = softmax_output.reshape(-1, 1)
        ret = np.diagflat(s) - np.dot(s, s.T)
        return ret

class CustomActivation(Activation):
    """Custom activation function"""
    fn: Callable

    def __init__(self, fn: Callable) -> None:
        self.fn = fn
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return self.fn(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        # TODO: AldyPy
        pass



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/main.py

import numpy as np
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from lib.activation import Tanh, Linear, ReLU, Sigmoid, Softmax
from lib.loss import CCE

from lib.weight_initializer import ZeroInitializer, UniformInitializer, NormalInitializer
from lib.neural import NeuralNetwork

initializer = NormalInitializer(seed=42)
print(initializer.initialize((3)))


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/nyoman-gana/00_initialize.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/services/.gitkeep



// -----------------------------------------------------------------------------------



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// plan

class NeuralNetwork:
	layers: List[NetworkLayer]
	weight_matrices: List[Matrix[Int]] # len(ini) = len(layers) - 1	
# Matrix[Int] -> m*n where m = len(layers[i]) and n = len(layers[i+1])
	gradient_matrices: List[Matrix[Int]]
	
	constructor(n: depth)
	
	setLossFunction(f: loss function = MSE)
	
	initialize_weights()
	
	show()
	
	plot_weights(arr: List[Int])
	
	plot_gradients(arr: List[Int])
	
	save(file: str)
	
	load(file: str)
	
	forward_propagation(arr: List[Input]) -> List[Output]

class FFNN:
	construct
	
	
	
	
	

class NetworkLayer:
	constructor(n: banyaknya node, f: fungsi aktivasi)


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// doc/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/neural.py

import numpy as np
import lib.activation as act
import lib.loss as loss
from lib.weight_initializer import WeightInitializer
import random
from typing import List

class NetworkLayer:
    """A layer in a neural network"""

    nodes: np.ndarray
    activation: act.Activation
    activated_nodes: np.ndarray

    def __init__(self, node_count: int, activation: act.Activation):
        """
        Initialize a network layer
        
        Args:
            node_count: Number of nodes in this layer
            activation: Activation function for this layer
        """
        self.activation = None if activation is None else activation()
        self.nodes = np.zeros(node_count)
        self.activated_nodes = np.zeros(node_count)



class NeuralNetwork:

    loss_function: loss.Loss
    layers: List[NetworkLayer]
    weights: List[np.ndarray] # 3D array
    gradients: List[np.ndarray] # 3D array
    bias_weights: List[np.ndarray] # 2D array
    bias_gradients: List[np.ndarray] # 2D array

    def __init__(
        self, 
        node_counts: List[int],
        activations: List[act.Activation],
        loss_function: loss.Loss,
        initialize_method: WeightInitializer
    ):
        if len(activations) != len(node_counts) - 1:
            raise Exception("Error: tried to declare NeuralNetwork with wrong amount of activation functions.")
        
        if len(node_counts) < 2:
            raise Exception("Error: tried to declare NeuralNetwork with less than 2 layers.")

        self.loss_function = loss_function

        activations = [None] + activations # input layer has no activation

        self.layers = [
            NetworkLayer(cnt, activation)
            for cnt, activation in zip(node_counts, activations)
        ]

        self.initialize_weights(initialize_method)
    
    def initialize_weights(self, initializer: WeightInitializer):

        self.weights = []        
        self.gradients = []
        self.bias_weights = []  
        
        for i in range(len(self.layers) - 1):
            next_layer_size = len(self.layers[i + 1].nodes)
            current_layer_size = len(self.layers[i].nodes)
            
            weight_matrix = initializer.initialize((next_layer_size, current_layer_size))
            self.weights.append(weight_matrix)
            
            bias_weights = initializer.initialize((next_layer_size,))
            self.bias_weights.append(bias_weights)
            
            self.gradients.append(np.zeros_like(weight_matrix))



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/weight_initializer.py

import numpy as np
from abc import ABC, abstractmethod


class WeightInitializer(ABC):
    """Abstract base class for neural network weight initialization."""
    
    @abstractmethod
    def initialize(self, shape):
        """
        Initialize weights according to a specific distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
        """
        pass


class ZeroInitializer(WeightInitializer):
    """Initialize weights with zeros."""
    
    def initialize(self, shape):
        """
        Initialize weights with zeros.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        Returns:
            numpy.ndarray: Zero-initialized weights.
        """
        return np.zeros(shape)


class UniformInitializer(WeightInitializer):
    """Initialize weights with random values from a uniform distribution."""
    
    def __init__(self, low=-0.05, high=0.05, seed=None):
        """
        Args:
            low (float): Lower bound of the uniform distribution.
            high (float): Upper bound of the uniform distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.low = low
        self.high = high
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a uniform distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.uniform(low=self.low, high=self.high, size=shape)


class NormalInitializer(WeightInitializer):
    """Initialize weights with random values from a normal distribution."""
    
    def __init__(self, mean=0.0, var=0.1, seed=None):
        """
        Args:
            mean (float): Mean of the normal distribution.
            var (float): Variance of the normal distribution.
            seed (int, optional): Random seed for reproducibility.
        """
        self.mean = mean
        self.std = np.sqrt(var)
        self.seed = seed
        
    def initialize(self, shape):
        """
        Initialize weights with random values from a normal distribution.
        
        Args:
            shape (tuple): Shape of the weight matrix/tensor to initialize.
            (rows, cols) for 2D weights, (rows, cols, channels) for 3D weights.
        """
        rng = np.random.RandomState(self.seed)
        return rng.normal(loc=self.mean, scale=self.std, size=shape)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/ffnn.py

from lib.neural import NeuralNetwork, NetworkLayer
from typing import List, Dict, Optional, Tuple
import numpy as np
import time
from tqdm import tqdm
from matplotlib import pyplot as plt
import pickle

class FFNN:
    
    network: NeuralNetwork
    learning_rate: float
    loss_history: Dict[str, List[float]]

    def __init__(self, network: NeuralNetwork, learning_rate: float = 0.01):
        """Initialize a Feed-Forward Neural Network.
        
        Args:
            network: The neural network architecture
            learning_rate: Learning rate for gradient descent
        """
        self.network = network
        self.learning_rate = learning_rate
        self.loss_history = {
            'train_loss': [],
            'val_loss': []
        }

    def forward_prop(self, x_batch: np.ndarray) -> np.ndarray:
        """Perform forward propagation through the network.
        
        Args:
            x_batch: Input data batch of shape (batch_size, input_features)
            
        Returns:
            Output predictions of shape (batch_size, output_features)
            
        Example:
            # For a network with 2 input features and 1 output:
            x_sample = np.array([[0.5, 0.8], [0.1, 0.2]])  # 2 samples, 2 features each
            predictions = model.forward_prop(x_sample)
            # predictions shape: (2, 1) e.g., [[0.75], [0.32]]
        """
        # Ensure x_batch is 2D
        if x_batch.ndim == 1:
            x_batch = x_batch.reshape(1, -1)
            
        batch_size = x_batch.shape[0]
        
        # Set input layer values
        self.network.layers[0].nodes = x_batch.T  # Transpose to (features, batch_size)
        self.network.layers[0].activated_nodes = self.network.layers[0].nodes  # No activation for input layer
        
        # Forward pass through each layer
        for i in range(1, len(self.network.layers)):
            current_layer = self.network.layers[i]
            prev_layer = self.network.layers[i-1]
            
            # Compute the weighted sum: weights * prev_activated_nodes + bias
            # Shape becomes (current_layer_nodes, batch_size)
            weighted_sum = np.dot(self.network.weights[i-1], prev_layer.activated_nodes)
            
            # Add bias (broadcasting across batch)
            bias_expanded = self.network.bias_weights[i-1].reshape(-1, 1)
            weighted_sum = weighted_sum + bias_expanded
            
            # Store pre-activation values
            current_layer.nodes = weighted_sum
            
            # Apply activation function
            if current_layer.activation is not None:
                current_layer.activated_nodes = current_layer.activation.function(weighted_sum)
            else:
                current_layer.activated_nodes = weighted_sum
        
        # Return output layer activations transposed back to (batch_size, output_features)
        return self.network.layers[-1].activated_nodes.T
    
    def back_prop(self, ):
        """TODO: implement backward prop by updating all:
            - weights[k][i][j]
            - gradients[k][i][j]
        """
    
    def update_weights(self) -> None:
        """Update weights using gradient descent.
        
        Example:
            # After computing gradients with back_prop
            model.update_weights()
            
            # This will update all weights and biases in the network:
            # For each weight matrix and bias vector:
            # w_new = w_old - learning_rate * gradient
        """
        for i in range(len(self.network.weights)):
            self.network.weights[i] -= self.learning_rate * self.network.gradients[i]
            self.network.bias_weights[i] -= self.learning_rate * self.network.bias_gradients[i]
    
    def fit(self, 
        x_train: np.ndarray, 
        y_train: np.ndarray,
        batch_size: int = 32,
        epochs: int = 10,
        validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,
        verbose: int = 1
    ) -> Dict[str, List[float]]:
        """Train the neural network using batched gradient descent.
        
        Args:
            x_train: Training data of shape (n_samples, n_features)
            y_train: Training targets of shape (n_samples, n_outputs)
            batch_size: Size of mini-batches
            epochs: Number of training epochs
            validation_data: Optional tuple of (x_val, y_val) for validation
            verbose: 0 for no output, 1 for progress bar and metrics
            
        Returns:
            History dictionary containing training and validation loss
            
        Example:
            # For a dataset with 1000 samples, 20 features, and 3 classes
            x_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each
            y_train = np.zeros((1000, 3))       # One-hot encoded targets for 3 classes
            # Set the correct class for each sample
            for i in range(1000):
                y_train[i, np.random.randint(0, 3)] = 1
                
            # Optional validation data
            x_val = np.random.rand(200, 20)
            y_val = np.zeros((200, 3))
            for i in range(200):
                y_val[i, np.random.randint(0, 3)] = 1
                
            # Train the model
            history = model.fit(
                x_train, 
                y_train,
                batch_size=32,
                epochs=50,
                validation_data=(x_val, y_val),
                verbose=1
            )
            
            # history is a dictionary with keys:
            # - 'train_loss': list of training loss values for each epoch
            # - 'val_loss': list of validation loss values for each epoch
        """
        # Ensure inputs are properly shaped
        if x_train.ndim == 1:
            x_train = x_train.reshape(-1, 1)
        if y_train.ndim == 1:
            y_train = y_train.reshape(-1, 1)
            
        num_samples = x_train.shape[0]
        
        # Reset loss history
        self.loss_history = {
            'train_loss': [],
            'val_loss': []
        }
        
        # Training loop
        for epoch in range(epochs):
            start_time = time.time()
            
            # Shuffle training data
            indices = np.random.permutation(num_samples)
            x_shuffled = x_train[indices]
            y_shuffled = y_train[indices]
            
            # Initialize epoch loss
            epoch_loss = 0.0
            
            # Create mini-batches
            num_batches = int(np.ceil(num_samples / batch_size))
            
            # Progress tracking
            if verbose == 1:
                batch_iterator = tqdm(range(num_batches), desc=f"Epoch {epoch+1}/{epochs}")
            else:
                batch_iterator = range(num_batches)
            
            # Process each batch
            for batch_idx in batch_iterator:
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, num_samples)
                
                batch_x = x_shuffled[start_idx:end_idx]
                batch_y = y_shuffled[start_idx:end_idx]
                
                # Forward and backward passes
                batch_loss = self.back_prop(batch_x, batch_y)
                
                # Update weights
                self.update_weights()
                
                # Update epoch loss (weighted by batch size)
                batch_weight = (end_idx - start_idx) / num_samples
                epoch_loss += batch_loss * batch_weight
                
                # Update progress bar
                if verbose == 1:
                    batch_iterator.set_postfix({"loss": f"{epoch_loss:.4f}"})
            
            # Record training loss
            self.loss_history['train_loss'].append(epoch_loss)
            
            # Validation phase
            val_loss = None
            if validation_data is not None:
                x_val, y_val = validation_data
                
                # Ensure proper shapes
                if x_val.ndim == 1:
                    x_val = x_val.reshape(-1, 1)
                if y_val.ndim == 1:
                    y_val = y_val.reshape(-1, 1)
                
                # Forward pass on validation data
                val_predictions = self.predict(x_val)
                
                # Calculate validation loss
                val_loss = self.network.loss_function.function(y_val.T, self.network.layers[-1].activated_nodes)
                self.loss_history['val_loss'].append(val_loss)
            
            # Print epoch summary
            if verbose == 1:
                epoch_time = time.time() - start_time
                if val_loss is not None:
                    print(f"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}")
                else:
                    print(f"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - loss: {epoch_loss:.4f}")
        
        return self.loss_history

    def predict(self, x: np.ndarray) -> np.ndarray:
        """Generate predictions for input samples.
        
        Args:
            x: Input data of shape (n_samples, n_features)
            
        Returns:
            Predictions of shape (n_samples, n_outputs)
            
        Example:
            # For a dataset with 10 samples, 20 features, and a model with 3 output classes
            x_test = np.random.rand(10, 20)
            
            # Make predictions
            predictions = model.predict(x_test)
            # predictions shape: (10, 3), e.g.:
            # [[0.1, 0.7, 0.2],  # Probabilities for sample 1
            #  [0.8, 0.1, 0.1],  # Probabilities for sample 2
            #  ...]
            
            # For classification, get the class with highest probability
            predicted_classes = np.argmax(predictions, axis=1)
            # predicted_classes: [1, 0, ...]
        """
        return self.forward_prop(x)

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> float:
        """Evaluate the model on test data.
        
        Args:
            x: Test data of shape (n_samples, n_features)
            y: True values of shape (n_samples, n_outputs)
            
        Returns:
            Loss value on test data
            
        Example:
            # For a test dataset with 100 samples, 20 features, and 3 classes
            x_test = np.random.rand(100, 20)
            y_test = np.zeros((100, 3))       # One-hot encoded targets
            for i in range(100):
                y_test[i, np.random.randint(0, 3)] = 1
                
            # Evaluate the model
            test_loss = model.evaluate(x_test, y_test)
            # test_loss is a float, e.g., 0.18
        """
        # Ensure inputs are properly shaped
        if x.ndim == 1:
            x = x.reshape(-1, 1)
        if y.ndim == 1:
            y = y.reshape(-1, 1)
            
        # Forward pass
        predictions = self.predict(x)
        
        # Calculate loss
        return self.network.loss_function.function(y.T, self.network.layers[-1].activated_nodes)
    
    def save(self, filepath: str) -> None:
        """Save the model to a file.
        
        Args:
            filepath: Path to save the model
            
        Example:
            # Save the trained model
            model.save("my_mnist_model.pkl")
        """
        with open(filepath, 'wb') as f:
            pickle.dump({
                'network': self.network,
                'learning_rate': self.learning_rate,
                'loss_history': self.loss_history
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'FFNN':
        """Load a model from a file.
        
        Args:
            filepath: Path to the saved model
            
        Returns:
            Loaded FFNN model
            
        Example:
            # Load a previously saved model
            loaded_model = FFNN.load("my_mnist_model.pkl")
            
            # Use the loaded model for predictions
            test_predictions = loaded_model.predict(x_test)
        """
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        model = cls(data['network'], data['learning_rate'])
        model.loss_history = data['loss_history']
        return model
    
    def plot_loss_history(self) -> None:
        """Plot the training and validation loss history.
        
        Example:
            # After training the model
            model.plot_loss_history()
            
            # This will display a graph showing:
            # - Training loss curve (blue line)
            # - Validation loss curve (if validation data was provided, orange line)
            # The x-axis represents epochs, and the y-axis represents loss values
        """
        if not self.loss_history['train_loss']:
            print("No training history to plot.")
            return
            
        plt.figure(figsize=(10, 6))
        plt.plot(self.loss_history['train_loss'], label='Training Loss', marker='o')
        
        if self.loss_history['val_loss']:
            plt.plot(self.loss_history['val_loss'], label='Validation Loss', marker='x')
        
        plt.title('Loss During Training')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/__init__.py



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/loss.py

import numpy as np
from abc import ABC, abstractmethod


class Loss(ABC):
    """Base class for loss functions"""
    @abstractmethod
    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    @abstractmethod
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        raise NotImplementedError


class MSE(Loss):
    """Mean Squared Error loss"""
    def __init__(self) -> None:
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return 0.5 * np.mean((y_true - y_pred) ** 2)
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return self.function(y_true, y_pred)


class BCE(Loss):
    """Binary Cross-Entropy loss"""
    def __init__(self) -> None:
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
        return self.function(y_true, y_pred)


class CCE:
    """Categorical Cross-Entropy loss"""
    def __init__(self):
        pass

    def function(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
        return loss
    
    def error(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:        
        return self.function(y_true, y_pred)

// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/lib/activation.py

import numpy as np
from abc import ABC, abstractmethod
from typing import Callable


class Activation(ABC):
    """Base class for activation functions"""
    @abstractmethod
    def function(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    @abstractmethod
    def derivative(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    

class Linear(Activation):
    """Linear activation function"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return x
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.ones_like(x)

class ReLU(Activation):
    """ReLU activation function: f(x) = max(0, x)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.maximum(0, x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return np.where(x > 0, 1, 0)


class Sigmoid(Activation):
    """Sigmoid activation function: f(x) = 1 / (1 + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        fx = self.function(x)
        return fx * (1 - fx)


class Tanh(Activation):
    """Hyperbolic tangent activation function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return np.tanh(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        return 1 - np.tanh(x) ** 2


class Softmax(Activation):
    """Softmax activation function: f(x_i) = e^x_i / sum(e^x_j)"""
    def __init__(self) -> None:
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        softmax_output = self.function(x)
        s = softmax_output.reshape(-1, 1)
        ret = np.diagflat(s) - np.dot(s, s.T)
        return ret

class CustomActivation(Activation):
    """Custom activation function"""
    fn: Callable

    def __init__(self, fn: Callable) -> None:
        self.fn = fn
        pass

    def function(self, x: np.ndarray) -> np.ndarray:
        return self.fn(x)
    
    def derivative(self, x: np.ndarray) -> np.ndarray:
        # TODO: AldyPy
        pass



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/main.py

import numpy as np
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from lib.activation import Tanh, Linear, ReLU, Sigmoid, Softmax
from lib.loss import CCE

from lib.weight_initializer import ZeroInitializer, UniformInitializer, NormalInitializer
from lib.neural import NeuralNetwork

initializer = NormalInitializer(seed=42)
print(initializer.initialize((3)))


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/.gitkeep



// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/playground/nyoman-gana/00_initialize.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


// -----------------------------------------------------------------------------------

// ----------------------------------------------------------------------------------------------------
// src/services/.gitkeep



// -----------------------------------------------------------------------------------

